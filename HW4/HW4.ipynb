{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import urllib\n",
    "import scipy.optimize\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import string\n",
    "import ast\n",
    "import math\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseData(fname):\n",
    "  for l in open(fname):\n",
    "    yield ast.literal_eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "### Just the first 5000 reviews\n",
    "\n",
    "print(\"Reading data...\")\n",
    "data = list(parseData(\"beer_50000.json\"))\n",
    "print(\"done\")\n",
    "\n",
    "# There's one review without text. Removing it\n",
    "first5000 = data[:5001]\n",
    "first5000.remove(first5000[3499])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ignore capitalization and remove punctuation\n",
    "\n",
    "bigramCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in first5000:\n",
    "    r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])\n",
    "    words = r.split()\n",
    "    for i in range(0, len(words)-1):\n",
    "        bigramCount[words[i] + ' ' + words[i+1]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostCommonBigrams = [(v, bigramCount[v]) for v in bigramCount]\n",
    "mostCommonBigrams = sorted(mostCommonBigrams, key=lambda x:-x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('with a', 4588),\n",
       " ('in the', 2597),\n",
       " ('of the', 2245),\n",
       " ('is a', 2057),\n",
       " ('on the', 2034)]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mostCommonBigrams[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = [x[0] for x in mostCommonBigrams[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sentiment analysis\n",
    "\n",
    "bigramId = dict(zip(bigrams, range(len(bigrams))))\n",
    "bigramSet = set(bigrams)\n",
    "\n",
    "def feature(datum):\n",
    "    feat = [0]*len(bigrams)\n",
    "    r = ''.join([c for c in datum['review/text'].lower() if not c in punctuation])\n",
    "    words = r.split()\n",
    "    for i in range(0, len(words)-1):\n",
    "        b = words[i] + ' ' + words[1]\n",
    "        if b in bigrams:\n",
    "            feat[bigramId[b]] += 1\n",
    "    feat.append(1) #offset\n",
    "    return feat\n",
    "\n",
    "X = [feature(d) for d in data]\n",
    "y = [d['review/overall'] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With regularization\n",
    "clf = linear_model.Ridge(1.0, fit_intercept=False)\n",
    "clf.fit(X, y)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.472180192210667\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(word, sentence):\n",
    "    return len([x for x in sentence.split() if x == word])/len(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(word, corpus):\n",
    "    value = 0\n",
    "    for s in corpus:\n",
    "        if word in s:\n",
    "            value += 1\n",
    "    if value == 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return math.log(len(corpus)/value, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [d['review/text'] for d in first5000]\n",
    "for i in range(0,len(corpus)):\n",
    "    r = ''.join([c for c in corpus[i].lower() if not c in punctuation])\n",
    "    corpus[i] = ''\n",
    "    for word in r.split():\n",
    "        corpus[i] += ' ' + word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foam\n",
      "idf = 0.9476909003526764\n",
      "tfidf = 0.03868126123888475\n",
      "\n",
      "smell\n",
      "idf = 0.38090666937325723\n",
      "tfidf = 0.007773605497413412\n",
      "\n",
      "banana\n",
      "idf = 1.5751183633689327\n",
      "tfidf = 0.0642905454436299\n",
      "\n",
      "lactic\n",
      "idf = 2.920818753952375\n",
      "tfidf = 0.11921709199805611\n",
      "\n",
      "tart\n",
      "idf = 1.0078885122130503\n",
      "tfidf = 0.020569153310470413\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for word in ['foam', 'smell', 'banana', 'lactic', 'tart']:\n",
    "    wordIDF = idf(word, corpus)\n",
    "    wordTF = tf(word, corpus[0])\n",
    "    print(word)\n",
    "    print('idf = ' + str(wordIDF))\n",
    "    print('tfidf = ' + str(wordTF*wordIDF))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "allWords = set(corpus[0].split() + corpus[1].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "review1 = []\n",
    "review2 = []\n",
    "for word in allWords:\n",
    "    wordIDF = idf(word, corpus)\n",
    "    review1TF = tf(word, corpus[0])\n",
    "    review2TF = tf(word, corpus[1])\n",
    "    review1.append(review1TF*wordIDF)\n",
    "    review2.append(review2TF*wordIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot = []\n",
    "r1magnitude = []\n",
    "r2magnitude = []\n",
    "for i in range(0, len(review1)):\n",
    "    dot.append(review1[i]*review2[i])\n",
    "    r1magnitude.append(review1[i]*review1[i])\n",
    "    r2magnitude.append(review2[i]*review2[i])\n",
    "dot = sum(dot)\n",
    "r1magnitude = math.sqrt(sum(r1magnitude))\n",
    "r2magnitude = math.sqrt(sum(r2magnitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine = dot/(r1magnitude*r2magnitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05716312389736702"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosSimilarity(v1, v2):\n",
    "    dot = []\n",
    "    v1magnitude = []\n",
    "    v2magnitude = []\n",
    "    for i in range(0, len(v1)):\n",
    "        dot.append(v1[i]*v2[i])\n",
    "        v1magnitude.append(v1[i]*v1[i])\n",
    "        v2magnitude.append(v2[i]*v2[i])\n",
    "    dot = sum(dot)\n",
    "    v1magnitude = math.sqrt(sum(v1magnitude))\n",
    "    v2magnitude = math.sqrt(sum(v2magnitude))\n",
    "    return dot/(v1magnitude*v2magnitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosSimilarities = []\n",
    "for i in range(1, len(corpus)):\n",
    "    review1 = []\n",
    "    review2 = []\n",
    "    allWords = set(corpus[0].split() + corpus[i].split())\n",
    "    for word in allWords:\n",
    "        wordIDF = idf(word, corpus)\n",
    "        review1TF = tf(word, corpus[0])\n",
    "        review2TF = tf(word, corpus[i])\n",
    "        review1.append(review1TF*wordIDF)\n",
    "        review2.append(review2TF*wordIDF)\n",
    "    cosSimilarities.append(cosSimilarity(review1, review2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2342"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosSimilarities.index(max(cosSimilarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profileName: spicelab\n",
      "beerId: 72146\n"
     ]
    }
   ],
   "source": [
    "print('profileName: ' + data[2342+1]['user/profileName'])\n",
    "print('beerId: ' + data[2342+1]['beer/beerId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Just take the most popular words...\n",
    "\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in first5000:\n",
    "  r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])\n",
    "  for w in r.split():\n",
    "    wordCount[w] += 1\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "words = [x[1] for x in counts[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sentiment analysis\n",
    "\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)\n",
    "\n",
    "wordsIDFs = []\n",
    "for w in words:\n",
    "    wordsIDFs.append(idf(w, corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(datum):\n",
    "    feat = []\n",
    "    for i in range(0, len(words)):\n",
    "        wordIDF = wordsIDFs[i]\n",
    "        wordTF = tf(words[i], corpus[first5000.index(datum)])\n",
    "        feat.append(wordTF*wordIDF)\n",
    "    feat.append(1) #offset\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [feature(d) for d in first5000]\n",
    "y = [d['review/overall'] for d in first5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With regularization\n",
    "clf = linear_model.Ridge(1.0, fit_intercept=False)\n",
    "clf.fit(X, y)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5208065538083951\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataCopy = data\n",
    "train = []\n",
    "validation = []\n",
    "test = []\n",
    "for i in range(0, 5000):\n",
    "    index = random.randint(0, len(dataCopy))\n",
    "    train.append(dataCopy[i])\n",
    "    dataCopy.remove(dataCopy[i])\n",
    "    index = random.randint(0, len(dataCopy))\n",
    "    validation.append(dataCopy[i])\n",
    "    dataCopy.remove(dataCopy[i])\n",
    "    index = random.randint(0, len(dataCopy))\n",
    "    test.append(dataCopy[i])\n",
    "    dataCopy.remove(dataCopy[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ignore capitalization and remove punctuation\n",
    "\n",
    "bigramCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in train:\n",
    "    r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])\n",
    "    words = r.split()\n",
    "    for i in range(0, len(words)-1):\n",
    "        bigramCount[words[i] + ' ' + words[i+1]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostCommonBigrams = [(v, bigramCount[v]) for v in bigramCount]\n",
    "mostCommonBigrams = sorted(mostCommonBigrams, key=lambda x:-x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramsNoPunct = [v[0] for v in mostCommonBigrams[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ignore capitalization and preserve punctuation\n",
    "import re\n",
    "\n",
    "def strip(s):\n",
    "    return s.strip()\n",
    "\n",
    "bigramCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in train:\n",
    "    r = ''.join([c for c in d['review/text'].lower()])\n",
    "    words = [item for item in map(strip, re.split(\"(\\W+)\", r)) if len(item) > 0]\n",
    "    for i in range(0, len(words)-1):\n",
    "        bigramCount[words[i] + ' ' + words[i+1]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostCommonBigrams = [(v, bigramCount[v]) for v in bigramCount]\n",
    "mostCommonBigrams = sorted(mostCommonBigrams, key=lambda x:-x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramsPunct = [v[0] for v in mostCommonBigrams[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = [bigramsNoPunct, bigramsPunct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Just take the most popular words without punctuation\n",
    "\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in train:\n",
    "  r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])\n",
    "  for w in r.split():\n",
    "    wordCount[w] += 1\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "wordsNoPunctuation = [x[1] for x in counts[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Just take the most popular words presereving punctiation\n",
    "\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in train:\n",
    "    r = ''.join([c for c in d['review/text'].lower()])\n",
    "    words = [item for item in map(strip, re.split(\"(\\W+)\", r)) if len(item) > 0]\n",
    "    for w in r.split():\n",
    "        wordCount[w] += 1\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "wordsPunctuation = [x[1] for x in counts[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = [wordsNoPunctuation, wordsPunctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "grams = [unigrams, bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusNoPunct = []\n",
    "corpusPunct = []\n",
    "for i in range(0, len(train)):\n",
    "    text = train[i]['review/text']\n",
    "    r = ''.join([c for c in text.lower() if not c in punctuation])\n",
    "    corpusNoPunct.append('')\n",
    "    for word in r.split():\n",
    "        corpusNoPunct[i] += ' ' + word\n",
    "        \n",
    "    corpusPunct.append('')\n",
    "    for word in ''.join([c for c in text.lower()]).split():\n",
    "        corpusPunct[i] += ' ' + word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [corpusNoPunct, corpusPunct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Getting IDFs\n",
    "\n",
    "IDFsTable = []\n",
    "# unigram no punct\n",
    "# unigram punct\n",
    "# bigram no punct\n",
    "# bigram punct\n",
    "\n",
    "gramsIds = []\n",
    "\n",
    "punct = 0\n",
    "for g in grams:\n",
    "    for j in g:\n",
    "        words = j\n",
    "        wordId = dict(zip(words, range(len(words))))\n",
    "        gramsIds.append(wordId)\n",
    "\n",
    "        wordsIDFs = []\n",
    "        for w in words:\n",
    "            wordsIDFs.append(idf(w, corpus[punct]))\n",
    "        IDFsTable.append(wordsIDFs)\n",
    "        punct = (punct + 1)%2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureUnigram(datum, punct):\n",
    "    feat = [0]*len(grams[0][punct])\n",
    "    if punct == 0:\n",
    "        r = ''.join([c for c in datum['review/text'].lower() if not c in punctuation])\n",
    "    else:\n",
    "        r = ''.join([c for c in datum['review/text'].lower()])\n",
    "    for w in r.split():\n",
    "        if w in grams[0][punct]:\n",
    "            feat[gramsIds[punct][w]] += 1\n",
    "    feat.append(1) #offset\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureBigrams(datum, punct):\n",
    "    feat = [0]*len(grams[1][punct])\n",
    "    if punct == 0:\n",
    "        r = ''.join([c for c in datum['review/text'].lower() if not c in punctuation])\n",
    "    else:\n",
    "        r = ''.join([c for c in datum['review/text'].lower()])\n",
    "    words = r.split()\n",
    "    for i in range(0, len(words)-1):\n",
    "        b = words[i] + ' ' + words[1]\n",
    "        if b in bigrams:\n",
    "            feat[gramsIds[2+punct][b]] += 1\n",
    "    feat.append(1) #offset\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureTFIDF(i, gram, punct):\n",
    "    #gram = 0 -> unigram\n",
    "    #gram = 1 -> bigram\n",
    "    sentence = corpus[punct][i]\n",
    "    if sentence == '':\n",
    "        return [0]*len(grams[gram][punct]) + [1]\n",
    "    \n",
    "    feat = []\n",
    "    for gramIndex in range(0, len(grams[gram][punct])):\n",
    "        currentGram = grams[gram][punct][gramIndex]\n",
    "        gramIDF = IDFsTable[2*gram+punct][gramsIds[2*gram+punct][currentGram]]\n",
    "        gramTF = tf(currentGram, sentence)\n",
    "        feat.append(gramTF*gramIDF)\n",
    "    feat.append(1) #offset\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running models\n",
    "thetas = []\n",
    "mses = []\n",
    "allModels = []\n",
    "for x in [0,1]: # unigram x bigram\n",
    "    for y in [0,1]: # noPunct x Punct\n",
    "        for z in [0,1]: # tfidf x counts\n",
    "            if z == 0:\n",
    "                X_train = [featureTFIDF(corpusIndex, x, y) for corpusIndex in range(0,len(train))]\n",
    "                X_validation = [featureTFIDF(corpusIndex, x, y) for corpusIndex in range(0,len(validation))]\n",
    "                X_test = [featureTFIDF(corpusIndex, x, y) for corpusIndex in range(0,len(test))]\n",
    "            else:\n",
    "                if x == 0:\n",
    "                    X_train = [featureUnigram(d, y) for d in train]\n",
    "                    X_validation = [featureUnigram(d, y) for d in validation]\n",
    "                    X_test = [featureUnigram(d, y) for d in test]\n",
    "                else:\n",
    "                    X_train = [featureBigrams(d, y) for d in train]\n",
    "                    X_validation = [featureBigrams(d, y) for d in validation]\n",
    "                    X_test = [featureBigrams(d, y) for d in test]\n",
    "                    \n",
    "            y_train = [d['review/overall'] for d in train]\n",
    "            y_validation = [d['review/overall'] for d in validation]\n",
    "            y_test = [d['review/overall'] for d in test]\n",
    "            \n",
    "            models = []\n",
    "            for c in [0.01, 0.1, 1, 10, 100]:\n",
    "                clf = linear_model.Ridge(c, fit_intercept=False)\n",
    "                clf.fit(X_train, y_train)\n",
    "                theta = clf.coef_\n",
    "                predictions = clf.predict(X_validation)\n",
    "                mse = mean_squared_error(y_validation, predictions)\n",
    "                models.append([clf, mse])\n",
    "            allModels.append(models)\n",
    "            models = sorted(models, key=lambda x:x[1])\n",
    "            bestModel = models[0][0]\n",
    "            thetas.append(bestModel.coef_)\n",
    "            testPreds = bestModel.predict(X_train)\n",
    "            mse = mean_squared_error(y_train, testPreds)\n",
    "            mses.append(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3827984754961387,\n",
       " 0.33818140493974114,\n",
       " 0.41449847824164754,\n",
       " 0.3567822877286595,\n",
       " 0.5791860002151384,\n",
       " 0.5791860002151384,\n",
       " 0.5791860002151384,\n",
       " 0.5791860002151384]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
